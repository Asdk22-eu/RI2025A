# -*- coding: utf-8 -*-
"""webscrapping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/Asdk22-eu/959d4792f81bf70ebcccc0b01e2251f5/webscrapping.ipynb

# **Webscraping**

## Parte 0: Planificar
Identificar los datos que quieres obtener.
Elegir el sitio web objetivo.
"""

!pip install beautifulsoup4 requests

"""##Parte 1: Entender el sitio web objetivo
Analizar la estructura de la página web a ser analizada.
Identificar los elementos HTML que contienen los datos bsuscados.
"""

from bs4 import BeautifulSoup

file = '/content/sample_data/data/12webcrawling/Rotisserie Chicken Recipe.html'

# Load the HTML file
with open(file, "r", encoding="utf-8") as file:
    html_content = file.read()

# Parse the HTML content with BeautifulSoup
soup = BeautifulSoup(html_content, "html.parser")

# Extracting the recipe title
title = soup.find("meta", {"property": "og:title"})["content"]
title

ingredients_section = soup.find_all("li", class_="mm-recipes-structured-ingredients__list-item")
for ingredient in ingredients_section:
    print(ingredient.text.strip())

"""##Parte 2: Obtener los datos deseados
Buscar dentro del contenido HTML y extraer la información.
"""

# Extracting the description
description = soup.find("meta", {"name": "description"})["content"]

# Extracting the ingredients
ingredients_section = soup.find_all("li", class_="mm-recipes-structured-ingredients__list-item")
ingredients = [ingredient.get_text().strip() for ingredient in ingredients_section]

# Extracting the instructions
instructions_section = soup.find_all("p", class_="comp mntl-sc-block mntl-sc-block-html")
instructions = [instruction.get_text().strip() for instruction in instructions_section]

# Extracting the nutrition information
nutrition_section = soup.find_all("span", class_="mm-recipes-nutrition-facts-label__nutrient-name mm-recipes-nutrition-facts-label__nutrient-name--has-postfix")
nutrition_facts = [fact.parent.get_text().strip().replace('\n', ' ') for fact in nutrition_section]

# Print the extracted information
print("Recipe Title:", title)
print("Description:", description)
print("Ingredients:")
for ingredient in ingredients:
    print("-", ingredient)
print("Instructions:")
for i, instruction in enumerate(instructions, 1):
    print(f"{i}. {instruction}")
print("Nutrition Facts:")
for fact in nutrition_facts:
    print("-", fact)

"""##Parte 3: Obtener enlaces relacionados
Encontrar links a otras recetas para completar el corpus
"""

# Find all the links to other recipes
recipe_links = soup.find_all("a", href=True)

# Filter and print only the links that are likely to be recipes
recipe_urls = []
for link in recipe_links:
    href = link['href']
    if "recipe" in href:
        recipe_urls.append(href)

# Print the recipe URLs
print("Linked Recipes:")
for url in recipe_urls:
    print(url)

"""## Parte 4: Hacer RAG con las recetas obtenidas
Una vez que se ha construido el corpus, implementar y desplegar RAG para realizar búsquedas en el corpus
"""

!pip install sentence-transformers faiss-cpu openai

import requests
from bs4 import BeautifulSoup

def extraer_receta(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    title = soup.find("meta", {"property": "og:title"})["content"] if soup.find("meta", {"property": "og:title"}) else "Sin título"
    description = soup.find("meta", {"name": "description"})["content"] if soup.find("meta", {"name": "description"}) else ""

    ingredients_section = soup.find_all("li", class_="mntl-structured-ingredients__list-item")
    ingredients = [i.get_text().strip() for i in ingredients_section]

    instructions_section = soup.find_all("p", class_="comp mntl-sc-block mntl-sc-block-html")
    instructions = [i.get_text().strip() for i in instructions_section]

    nutrition_section = soup.find_all("span", class_="mntl-nutrition-facts-label__text")
    nutrition = [n.parent.get_text().strip().replace('\n', ' ') for n in nutrition_section]

    # Juntar todo como texto
    content = f"{description}\n\nIngredientes:\n" + "\n".join(ingredients) + "\n\nInstrucciones:\n" + "\n".join(instructions)
    if nutrition:
        content += "\n\nInformación nutricional:\n" + "\n".join(nutrition)

    return {"title": title, "content": content}

#Lista de URLs obtenidas en Parte 3
urls = [
    "https://www.allrecipes.com/recipe/83557/rotisserie-chicken/",
  "https://www.allrecipes.com/authentication/login?regSource=3675&relativeRedirectUrl=%2Frecipe%2F93168%2Frotisserie-chicken%2F/account/add-recipe",
"https://www.myrecipes.com/favorites",
"https://www.allrecipes.com/authentication/logout?relativeRedirectUrl=%2Frecipe%2F93168%2Frotisserie-chicken%2F",
"https://www.magazines.com/allrecipes-magazine.html?utm_source=allrecipes.com&utm_medium=owned&utm_campaign=i111arr1w2661",
"https://www.allrecipes.com/recipes/86/world-cuisine/",
"https://www.allrecipes.com/kitchen-tips/",
"https://www.allrecipes.com/food-news-trends/",
"https://www.allrecipes.com/recipes/1642/everyday-cooking/",
"https://www.dotdashmeredith.com/brands/food-drink/allrecipes"

]

#  Corpus completo
corpus = [extraer_receta(url) for url in urls]

!pip install -q sentence-transformers faiss-cpu openai

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

# Modelo para embeddings
model = SentenceTransformer("all-MiniLM-L6-v2")

texts = [doc["content"] for doc in corpus]
titles = [doc["title"] for doc in corpus]

# Calcular embeddings
embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)

# Crear índice FAISS
index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(embeddings)

import openai
from openai import OpenAI
client = OpenAI(api_key = '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')

def rag_query(query, top_k=3):
    q_emb = model.encode([query])
    D, I = index.search(q_emb, top_k)
    contexto = corpus[I[0][0]]["content"]

    resp = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "Eres un chef experto que responde usando solo la receta dada."
},
            {"role": "user", "content": f"Contexto: {contexto}"},
            {"role": "user", "content": f"Pregunta: {query}"}
        ],
         temperature=0.2
    )
    return resp.choices[0].message.content

print(rag_query("¿Qué especias necesito para el pollo rostizado?", top_k=1))